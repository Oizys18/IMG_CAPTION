{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 전처리(Preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences :  [[1, 2, 3, 4, 6, 7]]\n",
      "word_index :  {'the': 1, 'earth': 2, 'is': 3, 'an': 4, 'awesome': 5, 'place': 6, 'live': 7}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "t = Tokenizer()\n",
    "fit_text = \"The earth is an awesome place live\"\n",
    "t.fit_on_texts([fit_text])\n",
    "\n",
    "test_text = \"The earth is an great place live\"\n",
    "sequences = t.texts_to_sequences([test_text])\n",
    "# sequences = t.texts_to_sequences([test_text])[0]\n",
    "\n",
    "print(\"sequences : \",sequences) # great는 단어 집합(vocabulary)에 없으므로 출력되지 않는다.\n",
    "print(\"word_index : \",t.word_index) # 단어 집합(vocabulary) 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad_sequence()는 정해준 길이보다 길이가 긴 샘플은 값을 일부 자르고, 정해준 길이보다 길이가 짧은 샘플은 값을 0으로 채웁니다.\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 첫번째 인자: 패딩을 진행할 데이터\n",
    "\n",
    "- maxlen: 정규화할 길이\n",
    "\n",
    "- padding\n",
    "\n",
    "    - pre: 앞에 0 채움\n",
    "    \n",
    "    - post: 뒤에 0 채움"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6],\n",
       "       [0, 7, 8]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequences([[1, 2, 3], [3, 4, 5, 6], [7, 8]], maxlen=3, padding='pre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 워드 임베딩(Word Embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 텍스트 내의 단어들을 밀집 벡터(dense vector, 임베딩 벡터)로 만드는 것\n",
    "\n",
    "- 밀집 벡터는 상대적으로 저차원이며 실수값을 가짐\n",
    "\n",
    "- 훈련 데이터로부터 학습함\n",
    "\n",
    "  - 초기에는 랜덤값을 가지지만, 인공 신경망의 가중치가 학습되는 방법과 같은 방식으로 값이 학습되며 변경됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Embedding()` 은 정수 인코딩 된 단어들을 입력받아 단어를 밀집 벡터로 만듦\n",
    "\n",
    "    - 인공 신경망 용어로는 임베딩 층을 만듦\n",
    "    \n",
    "    - 첫번째 인자 = 단어 집합의 크기. 즉, 총 단어의 개수\n",
    "    \n",
    "    - 두번째 인자 = 임베딩 벡터의 출력 차원. 결과로서 나오는 임베딩 벡터의 크기\n",
    "\n",
    "    - input_length = 입력 시퀀스의 길이\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
